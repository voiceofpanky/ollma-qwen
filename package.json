{
  "name": "my-ollama-server",
  "version": "1.0.0",
  "description": "Ollama + llama3 on Render with Node.js proxy",
  "main": "server.js",
  "type": "module",
  "scripts": {
    "start": "node server.js",
    "dev": "NODE_ENV=development node server.js"
  },
  "dependencies": {
    "axios": "^1.7.0",
    "cors": "^2.8.5",
    "dotenv": "^16.4.0",
    "express": "^4.19.0"
  }
}